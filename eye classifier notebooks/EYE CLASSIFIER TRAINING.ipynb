{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLOSE AND OPEN EYE CLASSIFIER TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have extract the eyes image from the original image we can use and train a classifier using a CNN for labeling eyes as open or close, using our dataset.\n",
    "\n",
    "This first model has 4 convolutional layers, with 16*n kernels of size 3x3, an output layer with a sigmoid activation funcion for binary classification, and Adam optimizer. Also a reduced LR callback has been used every 5 epochs without improvement in the validation accuracy.\n",
    "\n",
    "This model has 100% accuracy for train dataset, 97% for test dataset, and 95% for another public dataset download from kaggle.\n",
    "\n",
    "There is a 2nd model with also 4 convolutional layers, but the 1st layer with less kernels applied, and the callback function for reducing LR has been change to 10 epochs without improvement. It has better accuracy for our dataset, but worse for kaggle dataset.\n",
    "\n",
    "Finally, this models are saved to .h5 files, so they can be load and used in the eye detection code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import sklearn\n",
    "import Augmentor as aug\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brightness_augment(img, factor=0.5): \n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV) #convert to hsv\n",
    "    hsv = np.array(hsv, dtype=np.float64)\n",
    "    #hsv[:, :, 2] = hsv[:, :, 2] * (factor + np.random.uniform()) #scale channel V uniformly\n",
    "    hsv[:, :, 2] = hsv[:, :, 2] * (factor) #scale channel V uniformly\n",
    "    hsv[:, :, 2][hsv[:, :, 2] > 255] = 255 #reset out of range values\n",
    "    rgb = cv2.cvtColor(np.array(hsv, dtype=np.uint8), cv2.COLOR_HSV2RGB)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    if os.path.isdir(path) == False:\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "        else:\n",
    "            print (\"Successfully created the directory %s \" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(root):\n",
    "    #Method to load all the images and create X data and labels\n",
    "    data = []\n",
    "    for category in sorted(os.listdir(root)):\n",
    "        for file in sorted(os.listdir(os.path.join(root, category))):\n",
    "            data.append((category, os.path.join(root, category,  file)))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['class', 'file_path'])\n",
    "    df['class'] = LabelEncoder().fit_transform(df['class'].values)\n",
    "    photos, labels, names = list(), list(), list()\n",
    "    for index,row in df.iterrows():\n",
    "        photo = load_img(row['file_path'], target_size=(80, 80))\n",
    "        photo = img_to_array(photo)\n",
    "        photo = photo.astype(\"float32\") / 255\n",
    "        photos.append(photo)\n",
    "        labels.append(row['class'])\n",
    "        names.append(row['file_path'])\n",
    "    X = np.asarray(photos)\n",
    "    y = np.asarray(labels)\n",
    "    return X, y, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_size(root):\n",
    "    directory = os.path.join(root,\"rescaled\")\n",
    "    create_directory(directory)\n",
    "    for file in sorted(os.listdir(root)):\n",
    "        if os.path.isfile(os.path.join(root,file)):\n",
    "            if file.split(\".\")[1] in [\"jpg\",\"bmp\",\"tiff\"]:\n",
    "                path = os.path.join(root,file)\n",
    "                img = cv2.imread(path)\n",
    "                image = resize(img, (80, 80), preserve_range=True).astype(np.uint8)\n",
    "                name = os.path.join(directory,file)\n",
    "                cv2.imwrite(name, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallbackReduceLr(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.val_accuracy = np.Inf\n",
    "        self.accumulator = 0\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if self.accumulator == 5:\n",
    "            self.model.optimizer.lr = self.model.optimizer.lr*0.5\n",
    "            self.accumulator = 0\n",
    "            print(\"Changing Learning Rate to \", float(keras.backend.get_value(self.model.optimizer.lr)))\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs[\"val_accuracy\"]\n",
    "        if (current - self.val_accuracy) < 0.05:\n",
    "            self.accumulator = self.accumulator + 1\n",
    "            print(\"No change on val accuracy, increment accumulator to \", self.accumulator)\n",
    "        else:\n",
    "            self.accumulator = 0\n",
    "        self.val_accuracy = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallbackReduceLr2(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.val_accuracy = np.Inf\n",
    "        self.accumulator = 0\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if self.accumulator == 10:\n",
    "            self.model.optimizer.lr = self.model.optimizer.lr*0.5\n",
    "            self.accumulator = 0\n",
    "            print(\"Changing Learning Rate to \", float(keras.backend.get_value(self.model.optimizer.lr)))\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs[\"val_accuracy\"]\n",
    "        if (current - self.val_accuracy) < 0.05:\n",
    "            self.accumulator = self.accumulator + 1\n",
    "            print(\"No change on val accuracy, increment accumulator to \", self.accumulator)\n",
    "        else:\n",
    "            self.accumulator = 0\n",
    "        self.val_accuracy = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\train\\Open\\rescaled \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\test\\Open\\rescaled \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\train\\Closed\\rescaled \n"
     ]
    }
   ],
   "source": [
    "train_open_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\train\\Open'\n",
    "test_open_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\test\\Open'\n",
    "train_closed_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\train\\Closed'\n",
    "test_closed_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\test\\Closed'\n",
    "\n",
    "change_size(train_open_root)\n",
    "change_size(test_open_root)\n",
    "change_size(train_closed_root)\n",
    "change_size(test_closed_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 627 image(s) found.\n",
      "Output directory set to C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\train\\Open\\rescaled\\output.Operations: 5\n",
      "\t0: RotateRange (probability=0.8 max_left_rotation=-10 max_right_rotation=10 )\n",
      "\t1: Skew (probability=0.75 skew_type=TILT magnitude=0.6 )\n",
      "\t2: Distort (probability=0.75 grid_width=50 grid_height=50 magnitude=2 randomise_magnitude=True )\n",
      "\t3: Flip (probability=0.75 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t4: Zoom (probability=0.75 min_factor=1.1 max_factor=1.3 )\n",
      "Images: 627\n",
      "Classes: 1\n",
      "\tClass index: 0 Class label: rescaled \n",
      "Dimensions: 1\n",
      "\tWidth: 80 Height: 80\n",
      "Formats: 1\n",
      "\t JPEG\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=80x80 at 0x1EB0ACC93A0>: 100%|█| 6270/6270 [04:07<00:00, 25.33 Samples/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 117 image(s) found.\n",
      "Output directory set to C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\test\\Open\\rescaled\\output.Operations: 5\n",
      "\t0: RotateRange (probability=0.8 max_left_rotation=-10 max_right_rotation=10 )\n",
      "\t1: Skew (probability=0.75 skew_type=TILT magnitude=0.6 )\n",
      "\t2: Distort (probability=0.75 grid_width=50 grid_height=50 magnitude=2 randomise_magnitude=True )\n",
      "\t3: Flip (probability=0.75 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t4: Zoom (probability=0.75 min_factor=1.1 max_factor=1.3 )\n",
      "Images: 117\n",
      "Classes: 1\n",
      "\tClass index: 0 Class label: rescaled \n",
      "Dimensions: 1\n",
      "\tWidth: 80 Height: 80\n",
      "Formats: 1\n",
      "\t JPEG\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=80x80 at 0x1EB0BAC85B0>: 100%|█| 1170/1170 [00:44<00:00, 26.51 Samples/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 627 image(s) found.\n",
      "Output directory set to C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\train\\Closed\\rescaled\\output.Operations: 5\n",
      "\t0: RotateRange (probability=0.8 max_left_rotation=-10 max_right_rotation=10 )\n",
      "\t1: Skew (probability=0.75 skew_type=TILT magnitude=0.6 )\n",
      "\t2: Distort (probability=0.75 grid_width=50 grid_height=50 magnitude=2 randomise_magnitude=True )\n",
      "\t3: Flip (probability=0.75 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t4: Zoom (probability=0.75 min_factor=1.1 max_factor=1.3 )\n",
      "Images: 627\n",
      "Classes: 1\n",
      "\tClass index: 0 Class label: rescaled \n",
      "Dimensions: 1\n",
      "\tWidth: 80 Height: 80\n",
      "Formats: 1\n",
      "\t JPEG\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=80x80 at 0x1EBA91DEDF0>: 100%|█| 6270/6270 [04:14<00:00, 24.64 Samples/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 116 image(s) found.\n",
      "Output directory set to C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data\\test\\Closed\\rescaled\\output.Operations: 5\n",
      "\t0: RotateRange (probability=0.8 max_left_rotation=-10 max_right_rotation=10 )\n",
      "\t1: Skew (probability=0.75 skew_type=TILT magnitude=0.6 )\n",
      "\t2: Distort (probability=0.75 grid_width=50 grid_height=50 magnitude=2 randomise_magnitude=True )\n",
      "\t3: Flip (probability=0.75 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t4: Zoom (probability=0.75 min_factor=1.1 max_factor=1.3 )\n",
      "Images: 116\n",
      "Classes: 1\n",
      "\tClass index: 0 Class label: rescaled \n",
      "Dimensions: 1\n",
      "\tWidth: 80 Height: 80\n",
      "Formats: 1\n",
      "\t JPEG\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=80x80 at 0x1EB09C35D30>: 100%|█| 1160/1160 [00:53<00:00, 21.79 Samples/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\train\\Open \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\train\\Closed \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed \n"
     ]
    }
   ],
   "source": [
    "train_open_root_rescaled = os.path.join(train_open_root,\"rescaled\")\n",
    "test_open_root_rescaled = os.path.join(test_open_root,\"rescaled\")\n",
    "train_closed_root_rescaled = os.path.join(train_closed_root,\"rescaled\")\n",
    "test_closed_root_rescaled = os.path.join(test_closed_root,\"rescaled\")\n",
    "\n",
    "paths = [train_open_root_rescaled,test_open_root_rescaled,train_closed_root_rescaled,test_closed_root_rescaled]\n",
    "\n",
    "for path in paths:\n",
    "    length = len(os.listdir(path))\n",
    "    augmentor_train_pipeline = aug.Pipeline(path)\n",
    "    augmentor_train_pipeline.rotate(probability=0.8, max_left_rotation=10, max_right_rotation=10)\n",
    "    augmentor_train_pipeline.skew_tilt(probability=0.75,magnitude = 0.6)\n",
    "    augmentor_train_pipeline.random_distortion(probability=0.75, grid_width= 50, grid_height = 50, magnitude = 2)\n",
    "    augmentor_train_pipeline.flip_left_right(probability=0.75)\n",
    "    augmentor_train_pipeline.zoom(probability=0.75, min_factor=1.1, max_factor=1.3)\n",
    "    augmentor_train_pipeline.status()\n",
    "    augmentor_train_pipeline.sample(length*10)\n",
    "\n",
    "final_train_open_path = r\"C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\train\\Open\"\n",
    "final_train_closed_path = r\"C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\train\\Closed\"\n",
    "final_test_open_path = r\"C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\"\n",
    "final_test_closed_path = r\"C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\"\n",
    "\n",
    "create_directory(final_train_open_path)\n",
    "create_directory(final_train_closed_path)\n",
    "create_directory(final_test_open_path)\n",
    "create_directory(final_test_closed_path)\n",
    "\n",
    "for files in os.listdir(os.path.join(train_open_root_rescaled,\"Output\")):\n",
    "    source = os.path.join(os.path.join(train_open_root_rescaled,\"Output\"),files)\n",
    "    destination = os.path.join(final_train_open_path,files)\n",
    "    img = cv2.imread(source)\n",
    "    name1 = \"0\" + files\n",
    "    name2 = \"1\" + files\n",
    "    name3 = \"2\" + files\n",
    "    name4 = \"3\" + files\n",
    "    dest1 = os.path.join(final_train_open_path,name1)\n",
    "    dest2 = os.path.join(final_train_open_path,name2)\n",
    "    dest3 = os.path.join(final_train_open_path,name2)\n",
    "    dest4 = os.path.join(final_train_open_path,name2)\n",
    "    bright1 = brightness_augment(img, 1.5)\n",
    "    bright2 = brightness_augment(img, 2)\n",
    "    dark1 = brightness_augment(img, 0.75)\n",
    "    dark2 = brightness_augment(img, 0.6)\n",
    "    cv2.imwrite(dest1,bright1)\n",
    "    cv2.imwrite(dest2,bright2)\n",
    "    cv2.imwrite(dest3,dark1)\n",
    "    cv2.imwrite(dest4,dark2)\n",
    "    shutil.move(source, destination) \n",
    "shutil.rmtree(train_open_root_rescaled)\n",
    "\n",
    "for files in os.listdir(os.path.join(test_open_root_rescaled,\"Output\")):\n",
    "    source = os.path.join(os.path.join(test_open_root_rescaled,\"Output\"),files)\n",
    "    destination = os.path.join(final_test_open_path,files)\n",
    "    img = cv2.imread(source)\n",
    "    name1 = \"0\" + files\n",
    "    name2 = \"1\" + files\n",
    "    name3 = \"2\" + files\n",
    "    name4 = \"3\" + files\n",
    "    dest1 = os.path.join(final_test_open_path,name1)\n",
    "    dest2 = os.path.join(final_test_open_path,name2)\n",
    "    dest3 = os.path.join(final_test_open_path,name2)\n",
    "    dest4 = os.path.join(final_test_open_path,name2)\n",
    "    bright1 = brightness_augment(img, 1.5)\n",
    "    bright2 = brightness_augment(img, 2)\n",
    "    dark1 = brightness_augment(img, 0.75)\n",
    "    dark2 = brightness_augment(img, 0.6)\n",
    "    cv2.imwrite(dest1,bright1)\n",
    "    cv2.imwrite(dest2,bright2)\n",
    "    cv2.imwrite(dest3,dark1)\n",
    "    cv2.imwrite(dest4,dark2)\n",
    "    shutil.move(source, destination) \n",
    "shutil.rmtree(test_open_root_rescaled)\n",
    "\n",
    "for files in os.listdir(os.path.join(train_closed_root_rescaled,\"Output\")):\n",
    "    source = os.path.join(os.path.join(train_closed_root_rescaled,\"Output\"),files)\n",
    "    destination = os.path.join(final_train_closed_path,files)\n",
    "    img = cv2.imread(source)\n",
    "    name1 = \"0\" + files\n",
    "    name2 = \"1\" + files\n",
    "    name3 = \"2\" + files\n",
    "    name4 = \"3\" + files\n",
    "    dest1 = os.path.join(final_train_closed_path,name1)\n",
    "    dest2 = os.path.join(final_train_closed_path,name2)\n",
    "    dest3 = os.path.join(final_train_closed_path,name2)\n",
    "    dest4 = os.path.join(final_train_closed_path,name2)\n",
    "    bright1 = brightness_augment(img, 1.5)\n",
    "    bright2 = brightness_augment(img, 2)\n",
    "    dark1 = brightness_augment(img, 0.75)\n",
    "    dark2 = brightness_augment(img, 0.6)\n",
    "    cv2.imwrite(dest1,bright1)\n",
    "    cv2.imwrite(dest2,bright2)\n",
    "    cv2.imwrite(dest3,dark1)\n",
    "    cv2.imwrite(dest4,dark2)\n",
    "    shutil.move(source, destination) \n",
    "shutil.rmtree(train_closed_root_rescaled)\n",
    "    \n",
    "for files in os.listdir(os.path.join(test_closed_root_rescaled,\"Output\")):\n",
    "    source = os.path.join(os.path.join(test_closed_root_rescaled,\"Output\"),files)\n",
    "    destination = os.path.join(final_test_closed_path,files)\n",
    "    img = cv2.imread(source)\n",
    "    name1 = \"0\" + files\n",
    "    name2 = \"1\" + files\n",
    "    name3 = \"2\" + files\n",
    "    name4 = \"3\" + files\n",
    "    dest1 = os.path.join(final_test_closed_path,name1)\n",
    "    dest2 = os.path.join(final_test_closed_path,name2)\n",
    "    dest3 = os.path.join(final_test_closed_path,name2)\n",
    "    dest4 = os.path.join(final_test_closed_path,name2)\n",
    "    bright1 = brightness_augment(img, 1.5)\n",
    "    bright2 = brightness_augment(img, 2)\n",
    "    dark1 = brightness_augment(img, 0.75)\n",
    "    dark2 = brightness_augment(img, 0.6)\n",
    "    cv2.imwrite(dest1,bright1)\n",
    "    cv2.imwrite(dest2,bright2)\n",
    "    cv2.imwrite(dest3,dark1)\n",
    "    cv2.imwrite(dest4,dark2)\n",
    "    shutil.move(source, destination) \n",
    "shutil.rmtree(test_closed_root_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\train'\n",
    "test_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test'\n",
    "x_train, y_train, train_names = load_images(train_root)\n",
    "x_test, y_test, test_names = load_images(test_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 80, 80, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 40, 40, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 40, 40, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                204864    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 302,369\n",
      "Trainable params: 302,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu',padding='same', input_shape=(80, 80, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1129/1129 [==============================] - 101s 89ms/step - loss: 0.1259 - accuracy: 0.9531 - val_loss: 0.0314 - val_accuracy: 0.9854\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 2/100\n",
      "1129/1129 [==============================] - 102s 90ms/step - loss: 0.0337 - accuracy: 0.9886 - val_loss: 0.0063 - val_accuracy: 0.9984\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 3/100\n",
      "1129/1129 [==============================] - 107s 94ms/step - loss: 0.0213 - accuracy: 0.9923 - val_loss: 0.0049 - val_accuracy: 0.9981\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 4/100\n",
      "1129/1129 [==============================] - 108s 95ms/step - loss: 0.0141 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 5/100\n",
      "1129/1129 [==============================] - 106s 94ms/step - loss: 0.0121 - accuracy: 0.9961 - val_loss: 0.0024 - val_accuracy: 0.9992\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 6/100\n",
      "1129/1129 [==============================] - 107s 95ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 6.6572e-04 - val_accuracy: 0.9997\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 7/100\n",
      "1129/1129 [==============================] - 106s 94ms/step - loss: 0.0100 - accuracy: 0.9967 - val_loss: 3.5792e-04 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 8/100\n",
      "1129/1129 [==============================] - 107s 95ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0042 - val_accuracy: 0.9984\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 9/100\n",
      "1129/1129 [==============================] - 107s 95ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.0020 - val_accuracy: 0.9992\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 10/100\n",
      "1129/1129 [==============================] - 107s 94ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.0070 - val_accuracy: 0.9976\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 11/100\n",
      "Changing Learning Rate to  0.0005000000237487257\n",
      "1129/1129 [==============================] - 106s 94ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 2.0669e-04 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 12/100\n",
      "1129/1129 [==============================] - 107s 95ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 1.5373e-05 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 13/100\n",
      "1129/1129 [==============================] - 111s 98ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 4.1919e-06 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 14/100\n",
      "1129/1129 [==============================] - 117s 103ms/step - loss: 1.7355e-04 - accuracy: 0.9999 - val_loss: 1.9459e-07 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 15/100\n",
      "1129/1129 [==============================] - 117s 104ms/step - loss: 1.7374e-04 - accuracy: 0.9999 - val_loss: 4.0599e-07 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 16/100\n",
      "1129/1129 [==============================] - 115s 102ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 3.1612e-06 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 17/100\n",
      "1129/1129 [==============================] - 108s 95ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 7.5018e-04 - val_accuracy: 0.9997\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 18/100\n",
      "1129/1129 [==============================] - 107s 95ms/step - loss: 4.7433e-04 - accuracy: 0.9998 - val_loss: 6.7079e-05 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 19/100\n",
      "1129/1129 [==============================] - 111s 99ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 9.2367e-06 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 20/100\n",
      "1129/1129 [==============================] - 107s 94ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 1.9589e-06 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 21/100\n",
      "Changing Learning Rate to  0.0002500000118743628\n",
      "1129/1129 [==============================] - 112s 100ms/step - loss: 1.2937e-04 - accuracy: 0.9999 - val_loss: 3.1205e-07 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 22/100\n",
      "1129/1129 [==============================] - 109s 97ms/step - loss: 2.1709e-04 - accuracy: 0.9999 - val_loss: 8.4019e-08 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 23/100\n",
      "1129/1129 [==============================] - 106s 94ms/step - loss: 3.2299e-04 - accuracy: 0.9999 - val_loss: 3.2032e-07 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 24/100\n",
      "1129/1129 [==============================] - 108s 96ms/step - loss: 9.2013e-06 - accuracy: 1.0000 - val_loss: 1.5085e-07 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 25/100\n",
      "1129/1129 [==============================] - 109s 97ms/step - loss: 9.8318e-05 - accuracy: 1.0000 - val_loss: 1.6942e-06 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 26/100\n",
      "1129/1129 [==============================] - 107s 95ms/step - loss: 2.5497e-04 - accuracy: 0.9999 - val_loss: 4.5257e-05 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 27/100\n",
      "1129/1129 [==============================] - 107s 94ms/step - loss: 8.1675e-04 - accuracy: 0.9998 - val_loss: 2.0507e-06 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 28/100\n",
      "1129/1129 [==============================] - 107s 94ms/step - loss: 2.7548e-05 - accuracy: 1.0000 - val_loss: 2.3072e-08 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 29/100\n",
      "1129/1129 [==============================] - 107s 95ms/step - loss: 6.6097e-04 - accuracy: 0.9998 - val_loss: 4.8932e-09 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 30/100\n",
      "1129/1129 [==============================] - 99s 88ms/step - loss: 5.7596e-04 - accuracy: 0.9998 - val_loss: 1.0648e-06 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 31/100\n",
      "Changing Learning Rate to  0.0001250000059371814\n",
      "1129/1129 [==============================] - 98s 87ms/step - loss: 3.1722e-05 - accuracy: 1.0000 - val_loss: 1.3414e-08 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 32/100\n",
      "1129/1129 [==============================] - 98s 87ms/step - loss: 6.5411e-06 - accuracy: 1.0000 - val_loss: 3.5394e-09 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 33/100\n",
      "1129/1129 [==============================] - 99s 87ms/step - loss: 5.3674e-06 - accuracy: 1.0000 - val_loss: 1.1909e-09 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 34/100\n",
      "1129/1129 [==============================] - 98s 87ms/step - loss: 4.4020e-06 - accuracy: 1.0000 - val_loss: 1.6671e-09 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 35/100\n",
      "1129/1129 [==============================] - 98s 87ms/step - loss: 2.3231e-04 - accuracy: 0.9999 - val_loss: 7.0992e-08 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 36/100\n",
      "1129/1129 [==============================] - 99s 88ms/step - loss: 2.4287e-04 - accuracy: 0.9999 - val_loss: 2.3925e-08 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 37/100\n",
      "1129/1129 [==============================] - 98s 87ms/step - loss: 1.8012e-04 - accuracy: 1.0000 - val_loss: 4.7041e-09 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 38/100\n",
      "1129/1129 [==============================] - 99s 88ms/step - loss: 9.5673e-06 - accuracy: 1.0000 - val_loss: 4.7948e-09 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 39/100\n",
      "1129/1129 [==============================] - 96s 85ms/step - loss: 4.6542e-06 - accuracy: 1.0000 - val_loss: 1.6438e-09 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 40/100\n",
      "1129/1129 [==============================] - 96s 85ms/step - loss: 8.7044e-07 - accuracy: 1.0000 - val_loss: 3.0676e-10 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 41/100\n",
      "Changing Learning Rate to  6.25000029685907e-05\n",
      "1129/1129 [==============================] - 96s 85ms/step - loss: 6.4897e-06 - accuracy: 1.0000 - val_loss: 1.6784e-10 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 42/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 1.9585e-06 - accuracy: 1.0000 - val_loss: 6.8223e-11 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 43/100\n",
      "1129/1129 [==============================] - 107s 95ms/step - loss: 4.7391e-06 - accuracy: 1.0000 - val_loss: 2.6296e-11 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 44/100\n",
      "1129/1129 [==============================] - 119s 105ms/step - loss: 1.0715e-06 - accuracy: 1.0000 - val_loss: 1.2245e-11 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 45/100\n",
      "1129/1129 [==============================] - 109s 97ms/step - loss: 5.8912e-07 - accuracy: 1.0000 - val_loss: 2.2361e-11 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 46/100\n",
      "1129/1129 [==============================] - 116s 103ms/step - loss: 9.7108e-06 - accuracy: 1.0000 - val_loss: 9.1787e-14 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 47/100\n",
      "1129/1129 [==============================] - 115s 102ms/step - loss: 1.4463e-07 - accuracy: 1.0000 - val_loss: 1.4690e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 48/100\n",
      "1129/1129 [==============================] - 120s 106ms/step - loss: 1.9544e-07 - accuracy: 1.0000 - val_loss: 1.1013e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 49/100\n",
      "1129/1129 [==============================] - 119s 105ms/step - loss: 1.5038e-07 - accuracy: 1.0000 - val_loss: 8.9459e-14 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 50/100\n",
      "1129/1129 [==============================] - 119s 105ms/step - loss: 1.9633e-05 - accuracy: 1.0000 - val_loss: 7.2194e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 51/100\n",
      "Changing Learning Rate to  3.125000148429535e-05\n",
      "1129/1129 [==============================] - 123s 109ms/step - loss: 2.3749e-04 - accuracy: 1.0000 - val_loss: 2.1248e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 52/100\n",
      "1129/1129 [==============================] - 124s 110ms/step - loss: 3.7004e-07 - accuracy: 1.0000 - val_loss: 3.0654e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 53/100\n",
      "1129/1129 [==============================] - 119s 105ms/step - loss: 1.8185e-05 - accuracy: 1.0000 - val_loss: 1.8734e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 54/100\n",
      "1129/1129 [==============================] - 113s 100ms/step - loss: 1.7577e-05 - accuracy: 1.0000 - val_loss: 3.1843e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 55/100\n",
      "1129/1129 [==============================] - 117s 104ms/step - loss: 1.1848e-06 - accuracy: 1.0000 - val_loss: 4.7981e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 56/100\n",
      "1129/1129 [==============================] - 118s 104ms/step - loss: 6.3752e-07 - accuracy: 1.0000 - val_loss: 7.3666e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 57/100\n",
      "1129/1129 [==============================] - 117s 104ms/step - loss: 2.9816e-07 - accuracy: 1.0000 - val_loss: 7.4913e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 58/100\n",
      "1129/1129 [==============================] - 118s 105ms/step - loss: 5.2130e-07 - accuracy: 1.0000 - val_loss: 2.0956e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 59/100\n",
      "1129/1129 [==============================] - 115s 102ms/step - loss: 5.0587e-04 - accuracy: 1.0000 - val_loss: 9.0067e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 60/100\n",
      "1129/1129 [==============================] - 116s 102ms/step - loss: 4.6543e-07 - accuracy: 1.0000 - val_loss: 7.7655e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 61/100\n",
      "Changing Learning Rate to  1.5625000742147677e-05\n",
      "1129/1129 [==============================] - 119s 105ms/step - loss: 3.0184e-07 - accuracy: 1.0000 - val_loss: 6.7790e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 62/100\n",
      "1129/1129 [==============================] - 115s 102ms/step - loss: 6.3717e-07 - accuracy: 1.0000 - val_loss: 1.7639e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 63/100\n",
      "1129/1129 [==============================] - 122s 108ms/step - loss: 1.3560e-07 - accuracy: 1.0000 - val_loss: 1.9917e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 64/100\n",
      "1129/1129 [==============================] - 116s 102ms/step - loss: 3.8762e-07 - accuracy: 1.0000 - val_loss: 1.5380e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 65/100\n",
      "1129/1129 [==============================] - 126s 112ms/step - loss: 1.3812e-07 - accuracy: 1.0000 - val_loss: 1.3625e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 66/100\n",
      "1129/1129 [==============================] - 120s 106ms/step - loss: 1.5585e-07 - accuracy: 1.0000 - val_loss: 1.3445e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 67/100\n",
      "1129/1129 [==============================] - 100s 89ms/step - loss: 1.2153e-07 - accuracy: 1.0000 - val_loss: 1.1514e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 68/100\n",
      "1129/1129 [==============================] - 100s 89ms/step - loss: 1.3088e-07 - accuracy: 1.0000 - val_loss: 9.4804e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 69/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 3.8278e-07 - accuracy: 1.0000 - val_loss: 3.0432e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 70/100\n",
      "1129/1129 [==============================] - 98s 86ms/step - loss: 1.3777e-07 - accuracy: 1.0000 - val_loss: 4.0644e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 71/100\n",
      "Changing Learning Rate to  7.812500371073838e-06\n",
      "1129/1129 [==============================] - 102s 91ms/step - loss: 3.3199e-05 - accuracy: 1.0000 - val_loss: 4.1721e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 72/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 1.3084e-04 - accuracy: 1.0000 - val_loss: 7.0360e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 73/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 1.5016e-05 - accuracy: 1.0000 - val_loss: 8.4230e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 74/100\n",
      "1129/1129 [==============================] - 98s 86ms/step - loss: 7.9638e-08 - accuracy: 1.0000 - val_loss: 8.7981e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 75/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 4.2197e-04 - accuracy: 1.0000 - val_loss: 3.6312e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 76/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 5.4369e-06 - accuracy: 1.0000 - val_loss: 3.0442e-12 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 77/100\n",
      "1129/1129 [==============================] - 98s 87ms/step - loss: 9.2443e-07 - accuracy: 1.0000 - val_loss: 2.6440e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 78/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 5.8584e-08 - accuracy: 1.0000 - val_loss: 2.6465e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 79/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 1.1406e-07 - accuracy: 1.0000 - val_loss: 2.5767e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 80/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 1.6378e-07 - accuracy: 1.0000 - val_loss: 2.4349e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 81/100\n",
      "Changing Learning Rate to  3.906250185536919e-06\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 2.1036e-05 - accuracy: 1.0000 - val_loss: 1.6416e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 82/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 2.9610e-05 - accuracy: 1.0000 - val_loss: 1.1323e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 83/100\n",
      "1129/1129 [==============================] - 96s 85ms/step - loss: 1.6211e-04 - accuracy: 1.0000 - val_loss: 1.0711e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 84/100\n",
      "1129/1129 [==============================] - 96s 85ms/step - loss: 2.7616e-07 - accuracy: 1.0000 - val_loss: 1.0688e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 85/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 1.9336e-06 - accuracy: 1.0000 - val_loss: 1.1145e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 86/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 3.6300e-06 - accuracy: 1.0000 - val_loss: 9.9598e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 87/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 3.5263e-08 - accuracy: 1.0000 - val_loss: 9.9937e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 88/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 3.8477e-08 - accuracy: 1.0000 - val_loss: 1.0083e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 89/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 2.7932e-07 - accuracy: 1.0000 - val_loss: 9.9897e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 90/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 3.5780e-08 - accuracy: 1.0000 - val_loss: 1.0043e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 91/100\n",
      "Changing Learning Rate to  1.9531250927684596e-06\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 1.0928e-07 - accuracy: 1.0000 - val_loss: 1.0274e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 92/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 2.5850e-06 - accuracy: 1.0000 - val_loss: 1.0262e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 93/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 1.0009e-07 - accuracy: 1.0000 - val_loss: 1.0078e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 94/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 7.1722e-08 - accuracy: 1.0000 - val_loss: 9.9695e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 95/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 1.9993e-06 - accuracy: 1.0000 - val_loss: 1.0365e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 96/100\n",
      "1129/1129 [==============================] - 98s 86ms/step - loss: 3.1060e-07 - accuracy: 1.0000 - val_loss: 1.0592e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 97/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 5.7822e-08 - accuracy: 1.0000 - val_loss: 1.0544e-12 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 98/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 2.0764e-06 - accuracy: 1.0000 - val_loss: 9.2278e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 99/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 1.8824e-07 - accuracy: 1.0000 - val_loss: 9.2681e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 100/100\n",
      "1129/1129 [==============================] - 97s 86ms/step - loss: 6.2484e-08 - accuracy: 1.0000 - val_loss: 9.1581e-13 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n"
     ]
    }
   ],
   "source": [
    "change_lr_callback = CustomCallbackReduceLr2()\n",
    "history = model.fit(x_train, y_train, batch_size=30, epochs=100, verbose=1, validation_split=0.1, callbacks = [change_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__137.jpg_46a66481-b000-470b-9d60-3b2898dc99fb.jpg\n",
      "0.5315014\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__148.jpg_b8567930-264d-4bbf-af8b-87b583dc8d12.jpg\n",
      "0.86216235\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__163.jpg_3f8cf67b-a2d6-4b39-8cc2-684815b717ea.jpg\n",
      "0.9915544\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__168.jpg_395ab72d-d6d0-406a-b4c2-c2601ef49db7.jpg\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__168.jpg_45113875-8024-40bd-aa55-e22b7a47dbd5.jpg\n",
      "0.9435326\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__383.jpg_5e7266f0-f70d-471d-bf0f-19fd6c792bfd.jpg\n",
      "0.9384149\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__383.jpg_a838bae7-d510-4a3a-aa94-f331101d6de5.jpg\n",
      "0.93752563\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__656.jpg_5225be8b-74a2-487f-aa93-ed354b8c1594.jpg\n",
      "0.99528456\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_306cc2ae-3712-4cdf-ab85-2461f251b26f.jpg\n",
      "0.99873185\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_68834a02-3241-4dc2-a5a9-9e41bcf83dfb.jpg\n",
      "0.99892443\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_68e8487f-8a13-4003-8544-958c3cb76d3c.jpg\n",
      "0.9995576\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_6bd32f65-d2a9-4c6a-9f1a-7c8e301ed938.jpg\n",
      "0.9994788\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_6f59c1dd-a52a-4a45-93c5-a903a6621cb7.jpg\n",
      "0.99556166\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_6fb5983d-f5d5-4ec1-9e8e-f1a4789aea9c.jpg\n",
      "0.6620925\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_855d8fbd-367b-4d92-942c-bd6a42c7b7ea.jpg\n",
      "0.9982611\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_86714ded-3fa3-4629-aa03-9adec2557bee.jpg\n",
      "0.99947417\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_943598b1-9a63-4bf6-abe0-6af372039fe0.jpg\n",
      "0.99876326\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_a264b8c8-b223-4b8c-8a1d-4d4c5afeacbe.jpg\n",
      "0.99764967\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_a5dadf02-bb0e-4584-a6e2-b0d1abb34d7f.jpg\n",
      "0.9992714\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_aabe3cc4-7289-43ff-95a6-f2251ea4da49.jpg\n",
      "0.99851286\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_d6739a79-604a-41e5-aa39-7f8a40e653fc.jpg\n",
      "0.99847114\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original__666.jpg_ece4ee76-a684-44b4-bf9a-c087b856aa02.jpg\n",
      "0.98884493\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\0rescaled_original_eye_43.jpg_fedd3216-645e-4558-b3da-e50d34d8634a.jpg\n",
      "0.73969066\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\1rescaled_original__148.jpg_04b16b1c-de24-4816-9fa4-946434642476.jpg\n",
      "0.5760883\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\1rescaled_original__168.jpg_395ab72d-d6d0-406a-b4c2-c2601ef49db7.jpg\n",
      "0.9968037\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\rescaled_original__137.jpg_46a66481-b000-470b-9d60-3b2898dc99fb.jpg\n",
      "0.6945178\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\\rescaled_original__168.jpg_395ab72d-d6d0-406a-b4c2-c2601ef49db7.jpg\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\0rescaled_original__115.jpg_ba8353d7-b4b4-4c9d-b189-fda14fa02118.jpg\n",
      "0.0066637695\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\0rescaled_original__130.jpg_a0a8f258-7ebc-484f-b0ed-e7f8eac51500.jpg\n",
      "0.0010324717\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\0rescaled_original__152.jpg_b8795265-65ee-408b-9ce0-ae52354bb8b5.jpg\n",
      "5.4243576e-10\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\0rescaled_original__287.jpg_82659c6a-1739-447b-bed1-995beab43bdf.jpg\n",
      "1.2809076e-06\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\0rescaled_original__45.jpg_c58ddb23-be15-49d6-ae7e-5de36fb98223.jpg\n",
      "7.674368e-07\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\0rescaled_original__58.jpg_c83c07e6-63f1-452f-b08d-db5ee7693abc.jpg\n",
      "3.0139644e-11\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\0rescaled_original__597.jpg_76c3611d-a04a-490a-97b1-52f1fa412323.jpg\n",
      "1.0115324e-05\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\1rescaled_original__115.jpg_ba8353d7-b4b4-4c9d-b189-fda14fa02118.jpg\n",
      "0.00035327673\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\1rescaled_original__130.jpg_a0a8f258-7ebc-484f-b0ed-e7f8eac51500.jpg\n",
      "0.009005964\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\1rescaled_original__152.jpg_0a5242c2-d3dc-472b-b4ed-358b757ca8c1.jpg\n",
      "0.004844308\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\1rescaled_original__152.jpg_b8795265-65ee-408b-9ce0-ae52354bb8b5.jpg\n",
      "2.1440374e-09\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\1rescaled_original__287.jpg_82659c6a-1739-447b-bed1-995beab43bdf.jpg\n",
      "9.43931e-05\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\1rescaled_original__58.jpg_aa6ee985-c308-42f0-8bce-1382f2a0f1b7.jpg\n",
      "0.4880609\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\1rescaled_original__58.jpg_c83c07e6-63f1-452f-b08d-db5ee7693abc.jpg\n",
      "3.973136e-13\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\1rescaled_original__58.jpg_df81bff4-3457-4632-a136-98d62b799d7c.jpg\n",
      "0.0013489425\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\1rescaled_original__597.jpg_76c3611d-a04a-490a-97b1-52f1fa412323.jpg\n",
      "4.5579595e-06\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\1rescaled_original_eye_28.jpg_49f9e166-2be8-4535-893c-f12bec5f4ca5.jpg\n",
      "0.0029250681\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\rescaled_original__115.jpg_ba8353d7-b4b4-4c9d-b189-fda14fa02118.jpg\n",
      "1.8346484e-05\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\rescaled_original__130.jpg_a0a8f258-7ebc-484f-b0ed-e7f8eac51500.jpg\n",
      "0.001160711\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\rescaled_original__152.jpg_b8795265-65ee-408b-9ce0-ae52354bb8b5.jpg\n",
      "2.260505e-07\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\rescaled_original__287.jpg_82659c6a-1739-447b-bed1-995beab43bdf.jpg\n",
      "8.767559e-06\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\rescaled_original__58.jpg_c83c07e6-63f1-452f-b08d-db5ee7693abc.jpg\n",
      "1.1440573e-18\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\rescaled_original__58.jpg_df81bff4-3457-4632-a136-98d62b799d7c.jpg\n",
      "0.003921449\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\\rescaled_original__597.jpg_76c3611d-a04a-490a-97b1-52f1fa412323.jpg\n",
      "6.615037e-05\n",
      "219/219 [==============================] - 6s 29ms/step - loss: 0.0718 - accuracy: 0.9927\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3480\n",
      "           1       0.99      0.99      0.99      3510\n",
      "\n",
      "    accuracy                           0.99      6990\n",
      "   macro avg       0.99      0.99      0.99      6990\n",
      "weighted avg       0.99      0.99      0.99      6990\n",
      "\n",
      "[[3453   27]\n",
      " [  24 3486]]\n",
      "Time for prediction: 0.03708415614688857\n"
     ]
    }
   ],
   "source": [
    "y_predicted = []\n",
    "start_time = time.time()\n",
    "for i in range(len(y_test)):\n",
    "    out_probabilities = model.predict(x_test[i:i+1])\n",
    "    number_detected = 1 if out_probabilities[0][0] > 0.5 else 0\n",
    "    y_predicted.append(number_detected)\n",
    "    if number_detected != y_test[i]:\n",
    "        print(test_names[i])\n",
    "        print(out_probabilities[0][0])\n",
    "end_time = time.time()\n",
    "mean = (end_time - start_time) / len(y_test)\n",
    "a = model.evaluate(x_test, y_test)\n",
    "print(classification_report(y_test, y_predicted))\n",
    "print(confusion_matrix(y_test, y_predicted))\n",
    "print(\"Time for prediction: {}\".format(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176/1176 [==============================] - 31s 26ms/step - loss: 6.7375e-09 - accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     18810\n",
      "           1       1.00      1.00      1.00     18810\n",
      "\n",
      "    accuracy                           1.00     37620\n",
      "   macro avg       1.00      1.00      1.00     37620\n",
      "weighted avg       1.00      1.00      1.00     37620\n",
      "\n",
      "[[18810     0]\n",
      " [    0 18810]]\n",
      "Time for prediction: 0.0371910613246068\n"
     ]
    }
   ],
   "source": [
    "y_predicted = []\n",
    "start_time = time.time()\n",
    "for i in range(len(y_train)):\n",
    "    out_probabilities = model.predict(x_train[i:i+1])\n",
    "    number_detected = 1 if out_probabilities[0][0] > 0.5 else 0\n",
    "    y_predicted.append(number_detected)\n",
    "    if number_detected != y_train[i]:\n",
    "        print(train_names[i])\n",
    "end_time = time.time()\n",
    "mean = (end_time - start_time) / len(y_train)\n",
    "a = model.evaluate(x_train, y_train)\n",
    "print(classification_report(y_train, y_predicted))\n",
    "print(confusion_matrix(y_train, y_predicted))\n",
    "print(\"Time for prediction: {}\".format(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruebas_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas'\n",
    "x_pruebas, y_pruebas, pruebas_names = load_images(pruebas_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00016_0_1_0_0_0_01.png\n",
      "0.9999964\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00334_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00336_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00337_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00338_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00340_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00342_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00345_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00347_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00349_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00350_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00352_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00478_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00497_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00499_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00500_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00502_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00504_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00506_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00508_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00557_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00558_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00559_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00560_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00561_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00562_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00563_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00564_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00565_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00566_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00567_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00568_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00569_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00570_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00571_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00572_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00573_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00574_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00575_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00576_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00600_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00601_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00602_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00603_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00604_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00605_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00606_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0006_00607_0_1_0_1_0_01.png\n",
      "1.0\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0016_00617_1_0_0_0_1_01.png\n",
      "0.6386789\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0016_00618_1_0_0_0_1_01.png\n",
      "0.7234358\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0016_00620_1_0_0_0_1_01.png\n",
      "0.9995955\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0016_01639_1_0_0_0_1_01.png\n",
      "0.9485072\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0016_01691_1_0_0_0_1_01.png\n",
      "0.8411108\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas\\Closed\\s0016_01693_1_0_0_0_1_01.png\n",
      "0.9999997\n",
      "125/125 [==============================] - 3s 25ms/step - loss: 2.4854 - accuracy: 0.9865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99      2000\n",
      "           1       0.97      1.00      0.99      2000\n",
      "\n",
      "    accuracy                           0.99      4000\n",
      "   macro avg       0.99      0.99      0.99      4000\n",
      "weighted avg       0.99      0.99      0.99      4000\n",
      "\n",
      "[[1946   54]\n",
      " [   0 2000]]\n",
      "Time for prediction: 0.03590647178888321\n"
     ]
    }
   ],
   "source": [
    "y_predicted = []\n",
    "start_time = time.time()\n",
    "for i in range(len(y_pruebas)):\n",
    "    out_probabilities = model.predict(x_pruebas[i:i+1])\n",
    "    number_detected = 1 if out_probabilities[0][0] > 0.5 else 0\n",
    "    y_predicted.append(number_detected)\n",
    "    if number_detected != y_pruebas[i]:\n",
    "        print(pruebas_names[i])\n",
    "        print(out_probabilities[0][0])\n",
    "end_time = time.time()\n",
    "mean = (end_time - start_time) / len(y_pruebas)\n",
    "a = model.evaluate(x_pruebas, y_pruebas)\n",
    "print(classification_report(y_pruebas, y_predicted))\n",
    "print(confusion_matrix(y_pruebas, y_predicted))\n",
    "print(\"Time for prediction: {}\".format(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\YOLOV5\\video\\Closed\\eye_mauricio305.jpg\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.0179 - accuracy: 0.9986\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       326\n",
      "           1       1.00      1.00      1.00       371\n",
      "\n",
      "    accuracy                           1.00       697\n",
      "   macro avg       1.00      1.00      1.00       697\n",
      "weighted avg       1.00      1.00      1.00       697\n",
      "\n",
      "[[325   1]\n",
      " [  0 371]]\n"
     ]
    }
   ],
   "source": [
    "root2 = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\YOLOV5\\video'\n",
    "x_prueba2, y_prueba2, prueba2_names = load_images(root2)\n",
    "y_predicted = []\n",
    "for i in range(len(y_prueba2)):\n",
    "    out_probabilities = model.predict(x_prueba2[i:i+1])\n",
    "    number_detected = 1 if out_probabilities[0][0] > 0.05 else 0\n",
    "    y_predicted.append(number_detected)\n",
    "    if number_detected != y_prueba2[i]:\n",
    "        print(prueba2_names[i])\n",
    "a = model.evaluate(x_prueba2, y_prueba2)\n",
    "print(classification_report(y_prueba2, y_predicted))\n",
    "print(confusion_matrix(y_prueba2, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model/eye_classifier1_v3.h5\")\n",
    "#model2.save(\"model/eye_classifier2_v2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
