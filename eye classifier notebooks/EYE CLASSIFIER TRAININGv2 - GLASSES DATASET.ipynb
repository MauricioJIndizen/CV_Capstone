{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLOSE AND OPEN EYE CLASSIFIER TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have extract the eyes image from the original image we can use and train a classifier using a CNN for labeling eyes as open or close, using our dataset.\n",
    "\n",
    "This first model has 4 convolutional layers, with 16*n kernels of size 3x3, an output layer with a sigmoid activation funcion for binary classification, and Adam optimizer. Also a reduced LR callback has been used every 5 epochs without improvement in the validation accuracy.\n",
    "\n",
    "This model has 100% accuracy for train dataset, 97% for test dataset, and 95% for another public dataset download from kaggle.\n",
    "\n",
    "There is a 2nd model with also 4 convolutional layers, but the 1st layer with less kernels applied, and the callback function for reducing LR has been change to 10 epochs without improvement. It has better accuracy for our dataset, but worse for kaggle dataset.\n",
    "\n",
    "Finally, this models are saved to .h5 files, so they can be load and used in the eye detection code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import sklearn\n",
    "import Augmentor as aug\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.cm as cm\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brightness_augment(img, factor=0.5): \n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV) #convert to hsv\n",
    "    hsv = np.array(hsv, dtype=np.float64)\n",
    "    #hsv[:, :, 2] = hsv[:, :, 2] * (factor + np.random.uniform()) #scale channel V uniformly\n",
    "    hsv[:, :, 2] = hsv[:, :, 2] * (factor) #scale channel V uniformly\n",
    "    hsv[:, :, 2][hsv[:, :, 2] > 255] = 255 #reset out of range values\n",
    "    rgb = cv2.cvtColor(np.array(hsv, dtype=np.uint8), cv2.COLOR_HSV2RGB)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    if os.path.isdir(path) == False:\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "        else:\n",
    "            print (\"Successfully created the directory %s \" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(root):\n",
    "    #Method to load all the images and create X data and labels\n",
    "    data = []\n",
    "    for category in sorted(os.listdir(root)):\n",
    "        for file in sorted(os.listdir(os.path.join(root, category))):\n",
    "            data.append((category, os.path.join(root, category,  file)))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['class', 'file_path'])\n",
    "    df['class'] = LabelEncoder().fit_transform(df['class'].values)\n",
    "    photos, labels, names = list(), list(), list()\n",
    "    for index,row in df.iterrows():\n",
    "        photo = load_img(row['file_path'], color_mode = \"grayscale\", target_size=(80, 80))\n",
    "        photo = img_to_array(photo)\n",
    "        photo = photo.astype(\"float32\") / 255\n",
    "        photos.append(photo)\n",
    "        labels.append(row['class'])\n",
    "        names.append(row['file_path'])\n",
    "    X = np.asarray(photos)\n",
    "    y = np.asarray(labels)\n",
    "    return X, y, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_size(root):\n",
    "    directory = os.path.join(root,\"rescaled\")\n",
    "    create_directory(directory)\n",
    "    for file in sorted(os.listdir(root)):\n",
    "        if os.path.isfile(os.path.join(root,file)):\n",
    "            if file.split(\".\")[1] in [\"jpg\",\"bmp\",\"tiff\",\"png\"]:\n",
    "                path = os.path.join(root,file)\n",
    "                initial_img = cv2.imread(path)\n",
    "                gray = cv2.cvtColor(initial_img, cv2.COLOR_BGR2GRAY)\n",
    "                mask = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)[1]\n",
    "                img = cv2.inpaint(gray, mask, 3, cv2.INPAINT_TELEA) \n",
    "                image = resize(img, (80, 80), preserve_range=True).astype(np.uint8)\n",
    "                name = os.path.join(directory,file)\n",
    "                cv2.imwrite(name, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallbackReduceLr(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.val_accuracy = np.Inf\n",
    "        self.accumulator = 0\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if self.accumulator == 5:\n",
    "            self.model.optimizer.lr = self.model.optimizer.lr*0.5\n",
    "            self.accumulator = 0\n",
    "            print(\"Changing Learning Rate to \", float(keras.backend.get_value(self.model.optimizer.lr)))\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs[\"val_accuracy\"]\n",
    "        if (current - self.val_accuracy) < 0.05:\n",
    "            self.accumulator = self.accumulator + 1\n",
    "            print(\"No change on val accuracy, increment accumulator to \", self.accumulator)\n",
    "        else:\n",
    "            self.accumulator = 0\n",
    "        self.val_accuracy = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallbackReduceLr2(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.val_accuracy = np.Inf\n",
    "        self.accumulator = 0\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if self.accumulator == 10:\n",
    "            self.model.optimizer.lr = self.model.optimizer.lr*0.5\n",
    "            self.accumulator = 0\n",
    "            print(\"Changing Learning Rate to \", float(keras.backend.get_value(self.model.optimizer.lr)))\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs[\"val_accuracy\"]\n",
    "        if (current - self.val_accuracy) < 0.05:\n",
    "            self.accumulator = self.accumulator + 1\n",
    "            print(\"No change on val accuracy, increment accumulator to \", self.accumulator)\n",
    "        else:\n",
    "            self.accumulator = 0\n",
    "        self.val_accuracy = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 299x299\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    #array = cv2.cvtColor(array, cv2.COLOR_BGR2GRAY)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Load the original image\n",
    "    img = keras.preprocessing.image.load_img(img_path)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\train\\Open\\rescaled \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\test\\Open\\rescaled \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\train\\Closed\\rescaled \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\test\\Closed\\rescaled \n"
     ]
    }
   ],
   "source": [
    "train_open_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\train\\Open'\n",
    "test_open_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\test\\Open'\n",
    "train_closed_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\train\\Closed'\n",
    "test_closed_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\test\\Closed'\n",
    "\n",
    "change_size(train_open_root)\n",
    "change_size(test_open_root)\n",
    "change_size(train_closed_root)\n",
    "change_size(test_closed_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 2295 image(s) found.\n",
      "Output directory set to C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\train\\Open\\rescaled\\output.Operations: 5\n",
      "\t0: RotateRange (probability=0.8 max_left_rotation=-10 max_right_rotation=10 )\n",
      "\t1: Skew (probability=0.75 skew_type=TILT magnitude=0.6 )\n",
      "\t2: Distort (probability=0.75 grid_width=50 grid_height=50 magnitude=2 randomise_magnitude=True )\n",
      "\t3: Flip (probability=0.75 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t4: Zoom (probability=0.75 min_factor=1.1 max_factor=1.3 )\n",
      "Images: 2295\n",
      "Classes: 1\n",
      "\tClass index: 0 Class label: rescaled \n",
      "Dimensions: 1\n",
      "\tWidth: 80 Height: 80\n",
      "Formats: 2\n",
      "\t JPEG\n",
      "\t PNG\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=L size=80x80 at 0x1570E7C0EB0>: 100%|█| 22950/22950 [16:37<00:00, 23.00 Samples/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 449 image(s) found.\n",
      "Output directory set to C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\test\\Open\\rescaled\\output.Operations: 5\n",
      "\t0: RotateRange (probability=0.8 max_left_rotation=-10 max_right_rotation=10 )\n",
      "\t1: Skew (probability=0.75 skew_type=TILT magnitude=0.6 )\n",
      "\t2: Distort (probability=0.75 grid_width=50 grid_height=50 magnitude=2 randomise_magnitude=True )\n",
      "\t3: Flip (probability=0.75 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t4: Zoom (probability=0.75 min_factor=1.1 max_factor=1.3 )\n",
      "Images: 449\n",
      "Classes: 1\n",
      "\tClass index: 0 Class label: rescaled \n",
      "Dimensions: 1\n",
      "\tWidth: 80 Height: 80\n",
      "Formats: 2\n",
      "\t JPEG\n",
      "\t PNG\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=L size=80x80 at 0x15709FDE970>: 100%|█| 4490/4490 [03:18<00:00, 22.64 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 2295 image(s) found.\n",
      "Output directory set to C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\train\\Closed\\rescaled\\output.Operations: 5\n",
      "\t0: RotateRange (probability=0.8 max_left_rotation=-10 max_right_rotation=10 )\n",
      "\t1: Skew (probability=0.75 skew_type=TILT magnitude=0.6 )\n",
      "\t2: Distort (probability=0.75 grid_width=50 grid_height=50 magnitude=2 randomise_magnitude=True )\n",
      "\t3: Flip (probability=0.75 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t4: Zoom (probability=0.75 min_factor=1.1 max_factor=1.3 )\n",
      "Images: 2295\n",
      "Classes: 1\n",
      "\tClass index: 0 Class label: rescaled \n",
      "Dimensions: 1\n",
      "\tWidth: 80 Height: 80\n",
      "Formats: 2\n",
      "\t JPEG\n",
      "\t PNG\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=L size=80x80 at 0x1570E60B280>: 100%|█| 22950/22950 [17:22<00:00, 22.02 Samples/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 448 image(s) found.\n",
      "Output directory set to C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\data - copia\\test\\Closed\\rescaled\\output.Operations: 5\n",
      "\t0: RotateRange (probability=0.8 max_left_rotation=-10 max_right_rotation=10 )\n",
      "\t1: Skew (probability=0.75 skew_type=TILT magnitude=0.6 )\n",
      "\t2: Distort (probability=0.75 grid_width=50 grid_height=50 magnitude=2 randomise_magnitude=True )\n",
      "\t3: Flip (probability=0.75 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t4: Zoom (probability=0.75 min_factor=1.1 max_factor=1.3 )\n",
      "Images: 448\n",
      "Classes: 1\n",
      "\tClass index: 0 Class label: rescaled \n",
      "Dimensions: 1\n",
      "\tWidth: 80 Height: 80\n",
      "Formats: 2\n",
      "\t JPEG\n",
      "\t PNG\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=L size=80x80 at 0x15709AFCD90>: 100%|█| 4480/4480 [03:14<00:00, 23.02 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\train\\Open \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\train\\Closed \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed \n"
     ]
    }
   ],
   "source": [
    "train_open_root_rescaled = os.path.join(train_open_root,\"rescaled\")\n",
    "test_open_root_rescaled = os.path.join(test_open_root,\"rescaled\")\n",
    "train_closed_root_rescaled = os.path.join(train_closed_root,\"rescaled\")\n",
    "test_closed_root_rescaled = os.path.join(test_closed_root,\"rescaled\")\n",
    "\n",
    "paths = [train_open_root_rescaled,test_open_root_rescaled,train_closed_root_rescaled,test_closed_root_rescaled]\n",
    "\n",
    "for path in paths:\n",
    "    length = len(os.listdir(path))\n",
    "    augmentor_train_pipeline = aug.Pipeline(path)\n",
    "    augmentor_train_pipeline.rotate(probability=0.8, max_left_rotation=10, max_right_rotation=10)\n",
    "    augmentor_train_pipeline.skew_tilt(probability=0.75,magnitude = 0.6)\n",
    "    augmentor_train_pipeline.random_distortion(probability=0.75, grid_width= 50, grid_height = 50, magnitude = 2)\n",
    "    augmentor_train_pipeline.flip_left_right(probability=0.75)\n",
    "    augmentor_train_pipeline.zoom(probability=0.75, min_factor=1.1, max_factor=1.3)\n",
    "    augmentor_train_pipeline.status()\n",
    "    augmentor_train_pipeline.sample(length*10)\n",
    "\n",
    "final_train_open_path = r\"C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\train\\Open\"\n",
    "final_train_closed_path = r\"C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\train\\Closed\"\n",
    "final_test_open_path = r\"C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Open\"\n",
    "final_test_closed_path = r\"C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test\\Closed\"\n",
    "\n",
    "create_directory(final_train_open_path)\n",
    "create_directory(final_train_closed_path)\n",
    "create_directory(final_test_open_path)\n",
    "create_directory(final_test_closed_path)\n",
    "\n",
    "for files in os.listdir(os.path.join(train_open_root_rescaled,\"Output\")):\n",
    "    source = os.path.join(os.path.join(train_open_root_rescaled,\"Output\"),files)\n",
    "    destination = os.path.join(final_train_open_path,files)\n",
    "    img = cv2.imread(source)\n",
    "    name1 = \"0\" + files\n",
    "    name2 = \"1\" + files\n",
    "    name3 = \"2\" + files\n",
    "    name4 = \"3\" + files\n",
    "    dest1 = os.path.join(final_train_open_path,name1)\n",
    "    dest2 = os.path.join(final_train_open_path,name2)\n",
    "    dest3 = os.path.join(final_train_open_path,name2)\n",
    "    dest4 = os.path.join(final_train_open_path,name2)\n",
    "    bright1 = brightness_augment(img, 1.5)\n",
    "    bright2 = brightness_augment(img, 2)\n",
    "    dark1 = brightness_augment(img, 0.75)\n",
    "    dark2 = brightness_augment(img, 0.6)\n",
    "    cv2.imwrite(dest1,bright1)\n",
    "    cv2.imwrite(dest2,bright2)\n",
    "    cv2.imwrite(dest3,dark1)\n",
    "    cv2.imwrite(dest4,dark2)\n",
    "    shutil.move(source, destination) \n",
    "shutil.rmtree(train_open_root_rescaled)\n",
    "\n",
    "for files in os.listdir(os.path.join(test_open_root_rescaled,\"Output\")):\n",
    "    source = os.path.join(os.path.join(test_open_root_rescaled,\"Output\"),files)\n",
    "    destination = os.path.join(final_test_open_path,files)\n",
    "    img = cv2.imread(source)\n",
    "    name1 = \"0\" + files\n",
    "    name2 = \"1\" + files\n",
    "    name3 = \"2\" + files\n",
    "    name4 = \"3\" + files\n",
    "    dest1 = os.path.join(final_test_open_path,name1)\n",
    "    dest2 = os.path.join(final_test_open_path,name2)\n",
    "    dest3 = os.path.join(final_test_open_path,name2)\n",
    "    dest4 = os.path.join(final_test_open_path,name2)\n",
    "    bright1 = brightness_augment(img, 1.5)\n",
    "    bright2 = brightness_augment(img, 2)\n",
    "    dark1 = brightness_augment(img, 0.75)\n",
    "    dark2 = brightness_augment(img, 0.6)\n",
    "    cv2.imwrite(dest1,bright1)\n",
    "    cv2.imwrite(dest2,bright2)\n",
    "    cv2.imwrite(dest3,dark1)\n",
    "    cv2.imwrite(dest4,dark2)\n",
    "    shutil.move(source, destination) \n",
    "shutil.rmtree(test_open_root_rescaled)\n",
    "\n",
    "for files in os.listdir(os.path.join(train_closed_root_rescaled,\"Output\")):\n",
    "    source = os.path.join(os.path.join(train_closed_root_rescaled,\"Output\"),files)\n",
    "    destination = os.path.join(final_train_closed_path,files)\n",
    "    img = cv2.imread(source)\n",
    "    name1 = \"0\" + files\n",
    "    name2 = \"1\" + files\n",
    "    name3 = \"2\" + files\n",
    "    name4 = \"3\" + files\n",
    "    dest1 = os.path.join(final_train_closed_path,name1)\n",
    "    dest2 = os.path.join(final_train_closed_path,name2)\n",
    "    dest3 = os.path.join(final_train_closed_path,name2)\n",
    "    dest4 = os.path.join(final_train_closed_path,name2)\n",
    "    bright1 = brightness_augment(img, 1.5)\n",
    "    bright2 = brightness_augment(img, 2)\n",
    "    dark1 = brightness_augment(img, 0.75)\n",
    "    dark2 = brightness_augment(img, 0.6)\n",
    "    cv2.imwrite(dest1,bright1)\n",
    "    cv2.imwrite(dest2,bright2)\n",
    "    cv2.imwrite(dest3,dark1)\n",
    "    cv2.imwrite(dest4,dark2)\n",
    "    shutil.move(source, destination) \n",
    "shutil.rmtree(train_closed_root_rescaled)\n",
    "    \n",
    "for files in os.listdir(os.path.join(test_closed_root_rescaled,\"Output\")):\n",
    "    source = os.path.join(os.path.join(test_closed_root_rescaled,\"Output\"),files)\n",
    "    destination = os.path.join(final_test_closed_path,files)\n",
    "    img = cv2.imread(source)\n",
    "    name1 = \"0\" + files\n",
    "    name2 = \"1\" + files\n",
    "    name3 = \"2\" + files\n",
    "    name4 = \"3\" + files\n",
    "    dest1 = os.path.join(final_test_closed_path,name1)\n",
    "    dest2 = os.path.join(final_test_closed_path,name2)\n",
    "    dest3 = os.path.join(final_test_closed_path,name2)\n",
    "    dest4 = os.path.join(final_test_closed_path,name2)\n",
    "    bright1 = brightness_augment(img, 1.5)\n",
    "    bright2 = brightness_augment(img, 2)\n",
    "    dark1 = brightness_augment(img, 0.75)\n",
    "    dark2 = brightness_augment(img, 0.6)\n",
    "    cv2.imwrite(dest1,bright1)\n",
    "    cv2.imwrite(dest2,bright2)\n",
    "    cv2.imwrite(dest3,dark1)\n",
    "    cv2.imwrite(dest4,dark2)\n",
    "    shutil.move(source, destination) \n",
    "shutil.rmtree(test_closed_root_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\train'\n",
    "test_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\final_dataset\\test'\n",
    "x_train, y_train, train_names = load_images(train_root)\n",
    "x_test, y_test, test_names = load_images(test_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137700, 80, 80, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 80, 80, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 40, 40, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 40, 40, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                204864    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 302,081\n",
      "Trainable params: 302,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu',padding='same', input_shape=(80, 80, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2204/2204 [==============================] - 308s 139ms/step - loss: 0.0985 - accuracy: 0.9629 - val_loss: 0.0158 - val_accuracy: 0.9961\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 2/100\n",
      "2204/2204 [==============================] - 306s 139ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.0053 - val_accuracy: 0.9985\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 3/100\n",
      "2204/2204 [==============================] - 302s 137ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.0013 - val_accuracy: 0.9995\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 4/100\n",
      "2204/2204 [==============================] - 303s 137ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.0021 - val_accuracy: 0.9992\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 5/100\n",
      "2204/2204 [==============================] - 302s 137ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0053 - val_accuracy: 0.9980\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 6/100\n",
      "2204/2204 [==============================] - 302s 137ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0016 - val_accuracy: 0.9990\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 7/100\n",
      "2204/2204 [==============================] - 302s 137ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 5.5482e-04 - val_accuracy: 0.9998\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 8/100\n",
      "2204/2204 [==============================] - 304s 138ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 7.7412e-04 - val_accuracy: 0.9996\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 9/100\n",
      "2204/2204 [==============================] - 301s 137ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 2.1848e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 10/100\n",
      "2204/2204 [==============================] - 301s 136ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 3.6940e-04 - val_accuracy: 0.9998\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 11/100\n",
      "Changing Learning Rate to  0.0005000000237487257\n",
      "2204/2204 [==============================] - 302s 137ms/step - loss: 7.4109e-04 - accuracy: 0.9997 - val_loss: 5.3226e-04 - val_accuracy: 0.9998\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 12/100\n",
      "2204/2204 [==============================] - 317s 144ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 5.7511e-04 - val_accuracy: 0.9998\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 13/100\n",
      "2204/2204 [==============================] - 334s 152ms/step - loss: 2.2040e-04 - accuracy: 0.9999 - val_loss: 1.4906e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 14/100\n",
      "2204/2204 [==============================] - 319s 145ms/step - loss: 7.1603e-04 - accuracy: 0.9999 - val_loss: 0.0209 - val_accuracy: 0.9932\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 15/100\n",
      "2204/2204 [==============================] - 301s 136ms/step - loss: 8.5709e-04 - accuracy: 0.9997 - val_loss: 1.0676e-04 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 16/100\n",
      "2204/2204 [==============================] - 305s 138ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 9.4514e-05 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 17/100\n",
      "2204/2204 [==============================] - 330s 150ms/step - loss: 4.0524e-04 - accuracy: 0.9998 - val_loss: 3.2672e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 18/100\n",
      "2204/2204 [==============================] - 338s 153ms/step - loss: 3.5955e-04 - accuracy: 0.9999 - val_loss: 4.0129e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 19/100\n",
      "2204/2204 [==============================] - 332s 151ms/step - loss: 8.5352e-04 - accuracy: 0.9998 - val_loss: 1.0380e-04 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 20/100\n",
      "2204/2204 [==============================] - 321s 146ms/step - loss: 6.0129e-04 - accuracy: 0.9998 - val_loss: 1.7211e-06 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 21/100\n",
      "Changing Learning Rate to  0.0002500000118743628\n",
      "2204/2204 [==============================] - 322s 146ms/step - loss: 2.1994e-04 - accuracy: 0.9999 - val_loss: 4.3396e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 22/100\n",
      "2204/2204 [==============================] - 308s 140ms/step - loss: 4.5511e-05 - accuracy: 1.0000 - val_loss: 6.1688e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 23/100\n",
      "2204/2204 [==============================] - 305s 139ms/step - loss: 8.3922e-05 - accuracy: 1.0000 - val_loss: 8.3192e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 24/100\n",
      "2204/2204 [==============================] - 305s 138ms/step - loss: 3.1665e-04 - accuracy: 0.9999 - val_loss: 3.9722e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 25/100\n",
      "2204/2204 [==============================] - 308s 140ms/step - loss: 6.8951e-05 - accuracy: 1.0000 - val_loss: 3.5062e-04 - val_accuracy: 0.9998\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 26/100\n",
      "2204/2204 [==============================] - 369s 167ms/step - loss: 1.1220e-05 - accuracy: 1.0000 - val_loss: 2.4787e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 27/100\n",
      "2204/2204 [==============================] - 352s 160ms/step - loss: 1.1436e-04 - accuracy: 1.0000 - val_loss: 5.0486e-04 - val_accuracy: 0.9998\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 28/100\n",
      "2204/2204 [==============================] - 349s 159ms/step - loss: 2.8604e-04 - accuracy: 0.9999 - val_loss: 2.3464e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 29/100\n",
      "2204/2204 [==============================] - 347s 158ms/step - loss: 1.5691e-05 - accuracy: 1.0000 - val_loss: 3.9347e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 30/100\n",
      "2204/2204 [==============================] - 1113s 505ms/step - loss: 2.0184e-04 - accuracy: 1.0000 - val_loss: 7.4683e-05 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 31/100\n",
      "Changing Learning Rate to  0.0001250000059371814\n",
      "2204/2204 [==============================] - 309s 140ms/step - loss: 5.1668e-05 - accuracy: 1.0000 - val_loss: 1.0604e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 32/100\n",
      "2204/2204 [==============================] - 306s 139ms/step - loss: 3.7366e-06 - accuracy: 1.0000 - val_loss: 9.3170e-05 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 33/100\n",
      "2204/2204 [==============================] - 323s 147ms/step - loss: 4.3476e-05 - accuracy: 1.0000 - val_loss: 9.6682e-05 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 34/100\n",
      "2204/2204 [==============================] - 302s 137ms/step - loss: 6.6981e-05 - accuracy: 1.0000 - val_loss: 4.2282e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  4\n",
      "Epoch 35/100\n",
      "2204/2204 [==============================] - 291s 132ms/step - loss: 3.7953e-05 - accuracy: 1.0000 - val_loss: 2.7801e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  5\n",
      "Epoch 36/100\n",
      "2204/2204 [==============================] - 291s 132ms/step - loss: 1.0710e-04 - accuracy: 1.0000 - val_loss: 1.2686e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  6\n",
      "Epoch 37/100\n",
      "2204/2204 [==============================] - 291s 132ms/step - loss: 3.8714e-05 - accuracy: 1.0000 - val_loss: 6.9587e-05 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  7\n",
      "Epoch 38/100\n",
      "2204/2204 [==============================] - 308s 140ms/step - loss: 5.3228e-07 - accuracy: 1.0000 - val_loss: 1.2016e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  8\n",
      "Epoch 39/100\n",
      "2204/2204 [==============================] - 377s 171ms/step - loss: 2.2499e-06 - accuracy: 1.0000 - val_loss: 1.4426e-04 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  9\n",
      "Epoch 40/100\n",
      "2204/2204 [==============================] - 365s 165ms/step - loss: 3.0982e-04 - accuracy: 1.0000 - val_loss: 1.1777e-04 - val_accuracy: 1.0000\n",
      "No change on val accuracy, increment accumulator to  10\n",
      "Epoch 41/100\n",
      "Changing Learning Rate to  6.25000029685907e-05\n",
      "2204/2204 [==============================] - 351s 159ms/step - loss: 3.7180e-06 - accuracy: 1.0000 - val_loss: 1.9835e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  1\n",
      "Epoch 42/100\n",
      "2204/2204 [==============================] - 323s 146ms/step - loss: 7.7736e-07 - accuracy: 1.0000 - val_loss: 6.0812e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  2\n",
      "Epoch 43/100\n",
      "2204/2204 [==============================] - 305s 138ms/step - loss: 5.7077e-07 - accuracy: 1.0000 - val_loss: 2.3766e-04 - val_accuracy: 0.9999\n",
      "No change on val accuracy, increment accumulator to  3\n",
      "Epoch 44/100\n",
      "  31/2204 [..............................] - ETA: 4:57 - loss: 1.9631e-08 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-48d8e6aa76e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mchange_lr_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomCallbackReduceLr2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mchange_lr_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "change_lr_callback = CustomCallbackReduceLr2()\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model/eye_classifier1_v4.h5',\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "history = model.fit(x_train, y_train, batch_size=50, epochs=100, verbose=1, validation_split=0.2, callbacks = [change_lr_callback, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841/841 [==============================] - 24s 29ms/step - loss: 0.0354 - accuracy: 0.9973\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13440\n",
      "           1       1.00      1.00      1.00     13470\n",
      "\n",
      "    accuracy                           1.00     26910\n",
      "   macro avg       1.00      1.00      1.00     26910\n",
      "weighted avg       1.00      1.00      1.00     26910\n",
      "\n",
      "[[13392    48]\n",
      " [   25 13445]]\n",
      "Time for prediction: 0.04482897045620309\n"
     ]
    }
   ],
   "source": [
    "y_predicted = []\n",
    "start_time = time.time()\n",
    "for i in range(len(y_test)):\n",
    "    out_probabilities = model.predict(x_test[i:i+1])\n",
    "    number_detected = 1 if out_probabilities[0][0] > 0.5 else 0\n",
    "    y_predicted.append(number_detected)\n",
    "    #if number_detected != y_test[i]:\n",
    "    #    print(test_names[i])\n",
    "    #    print(out_probabilities[0][0])\n",
    "end_time = time.time()\n",
    "mean = (end_time - start_time) / len(y_test)\n",
    "a = model.evaluate(x_test, y_test)\n",
    "print(classification_report(y_test, y_predicted))\n",
    "print(confusion_matrix(y_test, y_predicted))\n",
    "print(\"Time for prediction: {}\".format(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = []\n",
    "start_time = time.time()\n",
    "for i in range(len(y_train)):\n",
    "    out_probabilities = model.predict(x_train[i:i+1])\n",
    "    number_detected = 1 if out_probabilities[0][0] > 0.5 else 0\n",
    "    y_predicted.append(number_detected)\n",
    "    if number_detected != y_train[i]:\n",
    "        print(train_names[i])\n",
    "end_time = time.time()\n",
    "mean = (end_time - start_time) / len(y_train)\n",
    "a = model.evaluate(x_train, y_train)\n",
    "print(classification_report(y_train, y_predicted))\n",
    "print(confusion_matrix(y_train, y_predicted))\n",
    "print(\"Time for prediction: {}\".format(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruebas_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\yolov5-face-master\\eye classifier notebooks\\pruebas'\n",
    "x_pruebas, y_pruebas, pruebas_names = load_images(pruebas_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 3s 24ms/step - loss: 0.7013 - accuracy: 0.9865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      2000\n",
      "           1       1.00      0.97      0.99      2000\n",
      "\n",
      "    accuracy                           0.99      4000\n",
      "   macro avg       0.99      0.99      0.99      4000\n",
      "weighted avg       0.99      0.99      0.99      4000\n",
      "\n",
      "[[2000    0]\n",
      " [  54 1946]]\n",
      "Time for prediction: 0.051617367506027224\n"
     ]
    }
   ],
   "source": [
    "y_predicted = []\n",
    "start_time = time.time()\n",
    "for i in range(len(y_pruebas)):\n",
    "    out_probabilities = model.predict(x_pruebas[i:i+1])\n",
    "    number_detected = 1 if out_probabilities[0][0] > 0.5 else 0\n",
    "    y_predicted.append(number_detected)\n",
    "    #if number_detected != y_pruebas[i]:\n",
    "        #print(pruebas_names[i])\n",
    "        #print(out_probabilities[0][0])\n",
    "end_time = time.time()\n",
    "mean = (end_time - start_time) / len(y_pruebas)\n",
    "a = model.evaluate(x_pruebas, y_pruebas)\n",
    "print(classification_report(y_pruebas, y_predicted))\n",
    "print(confusion_matrix(y_pruebas, y_predicted))\n",
    "print(\"Time for prediction: {}\".format(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\YOLOV5\\video\\Open\\rescaled \n",
      "Successfully created the directory C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\YOLOV5\\video\\Close\\rescaled \n"
     ]
    }
   ],
   "source": [
    "test2_open_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\YOLOV5\\video\\Open'\n",
    "test2_close_root = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\YOLOV5\\video\\Close'\n",
    "change_size(test2_open_root)\n",
    "change_size(test2_close_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\YOLOV5\\test\\closed\\eye_180.jpg\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\YOLOV5\\test\\closed\\eye_mauricio198.jpg\n",
      "C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\YOLOV5\\test\\open\\eye_mauricio306.jpg\n",
      "22/22 [==============================] - 1s 21ms/step - loss: 0.0449 - accuracy: 0.9943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       325\n",
      "           1       0.99      1.00      1.00       372\n",
      "\n",
      "    accuracy                           1.00       697\n",
      "   macro avg       1.00      1.00      1.00       697\n",
      "weighted avg       1.00      1.00      1.00       697\n",
      "\n",
      "[[323   2]\n",
      " [  1 371]]\n"
     ]
    }
   ],
   "source": [
    "root2 = r'C:\\Users\\guill\\Desktop\\INDIZEN\\Capstone\\YOLOV5\\test'\n",
    "\n",
    "x_prueba2, y_prueba2, prueba2_names = load_images(root2)\n",
    "y_predicted = []\n",
    "for i in range(len(y_prueba2)):\n",
    "    out_probabilities = model.predict(x_prueba2[i:i+1])\n",
    "    number_detected = 1 if out_probabilities[0][0] > 0.05 else 0\n",
    "    y_predicted.append(number_detected)\n",
    "    if number_detected != y_prueba2[i]:\n",
    "        print(prueba2_names[i])\n",
    "a = model.evaluate(x_prueba2, y_prueba2)\n",
    "print(classification_report(y_prueba2, y_predicted))\n",
    "print(confusion_matrix(y_prueba2, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model/eye_classifier1_v4.h5\")\n",
    "#model2.save(\"model/eye_classifier2_v2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
